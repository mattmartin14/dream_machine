{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark==3.4.0 delta-spark==2.4.0\n",
    "\n",
    "\"\"\"\n",
    "Author: Matt Martin\n",
    "Date: 2023-09-03\n",
    "Desc: Scratch pad testing delta lake\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.addPyFile(\"https://repo1.maven.org/maven2/io/delta/delta-core_2.12/0.8.0/delta-core_2.12-0.8.0.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table_if_exists(tbl_path) -> None:\n",
    "    \n",
    "    from delta import DeltaTable\n",
    "\n",
    "    try:\n",
    "        # Load the Delta table\n",
    "        delta_table = DeltaTable.forPath(spark, tbl_path)\n",
    "\n",
    "        # Delete the Delta table\n",
    "        delta_table.delete()\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_replace_delta_table(df, tbl_path) -> None:\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(tbl_path)\n",
    "    except Exception as e:\n",
    "        df.write.format(\"delta\").save(tbl_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_src_table() -> None:\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"hire_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        (\"Matt\", 20, datetime(2022,8,19)),\n",
    "        (\"Bill\", 35, datetime(2023,4,15)),\n",
    "        (\"Nancy\", 57, datetime(2022,4,23)),\n",
    "        (\"Rachel\", 19, datetime(2021,6,7)),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    create_or_replace_delta_table(df, \"./test/src_ppl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tgt_table() -> None:\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"hire_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        (\"Matt\", 18, datetime(2022,8,19)),\n",
    "        (\"Bill\", 22, datetime(2022,9,4)),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    create_or_replace_delta_table(df, \"./test/tgt_ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build source and target\n",
    "build_src_table()\n",
    "build_tgt_table()\n",
    "\n",
    "## sample\n",
    "\n",
    "## merge target\n",
    "\n",
    "## sample target again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source dataset\n",
      "+------+---+----------+\n",
      "|  name|age| hire_date|\n",
      "+------+---+----------+\n",
      "|Rachel| 19|2021-06-07|\n",
      "| Nancy| 57|2022-04-23|\n",
      "|  Bill| 35|2023-04-15|\n",
      "|  Matt| 20|2022-08-19|\n",
      "+------+---+----------+\n",
      "\n",
      "target_dataset\n",
      "+----+---+----------+\n",
      "|name|age| hire_date|\n",
      "+----+---+----------+\n",
      "|Bill| 22|2022-09-04|\n",
      "|Matt| 18|2022-08-19|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.read.format(\"delta\").load(\"./test/src_ppl\")\n",
    "tgt_df = spark.read.format(\"delta\").load(\"./test/tgt_ppl\")\n",
    "\n",
    "src_df.createTempView(\"src_tbl\")\n",
    "tgt_df.createTempView(\"tgt_tbl\")\n",
    "\n",
    "print('source dataset')\n",
    "src_df.show()\n",
    "print('target_dataset')\n",
    "tgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now do a merge and then reevaluate the target file\n",
    "sql = \"\"\"\n",
    "    MERGE into tgt_tbl as tgt\n",
    "        using src_tbl as src\n",
    "            on tgt.name = src.name\n",
    "        when matched then update\n",
    "            set tgt.age = src.age, tgt.hire_date = src.hire_date\n",
    "        when not matched then\n",
    "            insert (name, age, hire_date)\n",
    "            values (src.name, src.age, src.hire_date)\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age| hire_date|\n",
      "+------+---+----------+\n",
      "|  Bill| 35|2023-04-15|\n",
      "|  Matt| 20|2022-08-19|\n",
      "| Nancy| 57|2022-04-23|\n",
      "|Rachel| 19|2021-06-07|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age| hire_date|\n",
      "+------+---+----------+\n",
      "|  Bill| 35|2023-04-15|\n",
      "|  Matt| 20|2022-08-19|\n",
      "| Nancy| 57|2022-04-23|\n",
      "|Rachel| 20|2021-06-07|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Update tgt_tbl set age = 20 where name = 'Rachel'\")\n",
    "tgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age| hire_date|\n",
      "+------+---+----------+\n",
      "|  Bill| 35|2023-04-15|\n",
      "|  Matt| 20|2022-08-19|\n",
      "| Nancy| 57|2022-04-23|\n",
      "|Rachel| 20|2021-06-07|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"./test/tgt_ppl\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write perm results back out \n",
    "create_or_replace_delta_table(df, \"./test/tgt_ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age| hire_date|\n",
      "+------+---+----------+\n",
      "|  Bill| 35|2023-04-15|\n",
      "|  Matt| 20|2022-08-19|\n",
      "| Nancy| 57|2022-04-23|\n",
      "|Rachel| 20|2021-06-07|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createTempView(\"persons\")\n",
    "spark.sql(\"SELECT * FROM persons\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### you can do direct writes to delta tables without having to put in a df first\n",
    "\n",
    "sql = \"\"\"\n",
    "    MERGE INTO delta.`./test/tgt_ppl` AS TGT\n",
    "        USING delta.`./test/src_ppl` as SRC\n",
    "            ON TGT.id = SRC.id\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (id)\n",
    "            VALUES (SRC.id)\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "\n",
    "spark.sql(\"CREATE OR REPLACE TABLE persons_copy USING delta location './test/persons_copy' AS SELECT * FROM persons\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE test2_ppl USING delta LOCATION './test/tgt_ppl' AS SELECT * FROM t1\")\n",
    "\n",
    "spark.sql(\"UPDATE delta.`./test/tgt_ppl` set id = 12 where id = 7\")\n",
    "\n",
    "spark.sql(\"delete from delta.`./test/tgt_ppl` where id in (6,8)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
