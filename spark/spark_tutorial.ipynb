{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 06:34:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Matt Martin\n",
    "Date: 10/24/2023\n",
    "Desc: Simple demo using spark \n",
    "\"\"\"\n",
    "\n",
    "## create the spark connection/instance\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./test_dw\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#generate some data\n",
    "data1 = [\n",
    "     {'name': 'Matt', 'age':12}\n",
    "    ,{'name': 'Alex', 'age':14}\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data1)\n",
    "df1.write.mode(\"overwrite\").parquet('./persons1.parquet')\n",
    "\n",
    "data2 = [\n",
    "     {'name': 'Tom', 'age':19}\n",
    "    ,{'name': 'Sandy', 'age':31}\n",
    "    ,{'name': 'Matt', 'age': 15}\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.write.mode(\"overwrite\").parquet('./persons2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign views to each dataframe\n",
    "df1.createOrReplaceTempView(\"persons1\")\n",
    "df2.createOrReplaceTempView(\"persons2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run some sql to join the data and write out to a file\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "     COALESCE(p1.name, p2.name) as name\n",
    "    ,COALESCE(p1.age, p2.age) as age1\n",
    "    ,p2.age as other_age\n",
    "FROM persons1 as p1\n",
    "    FULL OUTER JOIN persons2 as p2\n",
    "        ON p1.name = p2.name\n",
    "\"\"\"\n",
    "spark.sql(sql).write.mode(\"overwrite\").parquet('./persons_combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "create or replace temp view test\n",
    "as\n",
    "SELECT\n",
    "     COALESCE(p1.name, p2.name) as name\n",
    "    ,COALESCE(p1.age, p2.age) as age1\n",
    "    ,p2.age as other_age\n",
    "FROM persons1 as p1\n",
    "    FULL OUTER JOIN persons2 as p2\n",
    "        ON p1.name = p2.name\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| name|age1|other_age|\n",
      "+-----+----+---------+\n",
      "| Alex|  14|     null|\n",
      "| Matt|  12|       15|\n",
      "|Sandy|  31|       31|\n",
      "|  Tom|  19|       19|\n",
      "+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.saveAsTable(\"test_tbl3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 12|Matt|\n",
      "| 14|Alex|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test_tbl3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| name|age1|other_age|\n",
      "+-----+----+---------+\n",
      "| Alex|  14|     null|\n",
      "| Matt|  12|       15|\n",
      "|Sandy|  31|       31|\n",
      "|  Tom|  19|       19|\n",
      "+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validate what the output was\n",
    "spark.read.parquet('./persons_combined.parquet').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
