{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The free tier for GBQ covers:\n",
    "1. Loading data to a table from a data frame\n",
    "2. Select queries\n",
    "\n",
    "What is not covered and requires billing:\n",
    "- DML's e.g. insert/update/delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loading data\n",
    "\n",
    "# Define your BigQuery dataset and table names\n",
    "dataset_id = 'ds1_test'\n",
    "table_name = 'users2'\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = '~/test_dummy_data/gbq/test_data.csv'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Don</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id user_name\n",
       "0        1       Bob\n",
       "1        2      Bill\n",
       "2        3      Kate\n",
       "3        4      Matt\n",
       "4        5       Don"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=test-matt-219200, location=US, id=9f0b7b62-063d-40f7-b152-b5e95c8b05cf>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loads CSV to table\n",
    "table_ref = client.dataset(dataset_id).table(table_name)\n",
    "client.load_table_from_dataframe(df, table_ref).result()\n",
    "#job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load_table_from_dataframe in module google.cloud.bigquery.client:\n",
      "\n",
      "load_table_from_dataframe(dataframe: 'pandas.DataFrame', destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, str], num_retries: int = 6, job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.load.LoadJobConfig] = None, parquet_compression: str = 'snappy', timeout: Union[NoneType, float, Tuple[float, float]] = None) -> google.cloud.bigquery.job.load.LoadJob method of google.cloud.bigquery.client.Client instance\n",
      "    Upload the contents of a table from a pandas DataFrame.\n",
      "    \n",
      "    Similar to :meth:`load_table_from_uri`, this method creates, starts and\n",
      "    returns a :class:`~google.cloud.bigquery.job.LoadJob`.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        REPEATED fields are NOT supported when using the CSV source format.\n",
      "        They are supported when using the PARQUET source format, but\n",
      "        due to the way they are encoded in the ``parquet`` file,\n",
      "        a mismatch with the existing table schema can occur, so\n",
      "        REPEATED fields are not properly supported when using ``pyarrow<4.0.0``\n",
      "        using the parquet format.\n",
      "    \n",
      "        https://github.com/googleapis/python-bigquery/issues/19\n",
      "    \n",
      "    Args:\n",
      "        dataframe:\n",
      "            A :class:`~pandas.DataFrame` containing the data to load.\n",
      "        destination:\n",
      "            The destination table to use for loading the data. If it is an\n",
      "            existing table, the schema of the :class:`~pandas.DataFrame`\n",
      "            must match the schema of the destination table. If the table\n",
      "            does not yet exist, the schema is inferred from the\n",
      "            :class:`~pandas.DataFrame`.\n",
      "    \n",
      "            If a string is passed in, this method attempts to create a\n",
      "            table reference from a string using\n",
      "            :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      "    \n",
      "    Keyword Arguments:\n",
      "        num_retries: Number of upload retries.\n",
      "        job_id: Name of the job.\n",
      "        job_id_prefix:\n",
      "            The user-provided prefix for a randomly generated\n",
      "            job ID. This parameter will be ignored if a ``job_id`` is\n",
      "            also given.\n",
      "        location:\n",
      "            Location where to run the job. Must match the location of the\n",
      "            destination table.\n",
      "        project:\n",
      "            Project ID of the project of where to run the job. Defaults\n",
      "            to the client's project.\n",
      "        job_config:\n",
      "            Extra configuration options for the job.\n",
      "    \n",
      "            To override the default pandas data type conversions, supply\n",
      "            a value for\n",
      "            :attr:`~google.cloud.bigquery.job.LoadJobConfig.schema` with\n",
      "            column names matching those of the dataframe. The BigQuery\n",
      "            schema is used to determine the correct data type conversion.\n",
      "            Indexes are not loaded.\n",
      "    \n",
      "            By default, this method uses the parquet source format. To\n",
      "            override this, supply a value for\n",
      "            :attr:`~google.cloud.bigquery.job.LoadJobConfig.source_format`\n",
      "            with the format name. Currently only\n",
      "            :attr:`~google.cloud.bigquery.job.SourceFormat.CSV` and\n",
      "            :attr:`~google.cloud.bigquery.job.SourceFormat.PARQUET` are\n",
      "            supported.\n",
      "        parquet_compression:\n",
      "            [Beta] The compression method to use if intermittently\n",
      "            serializing ``dataframe`` to a parquet file.\n",
      "    \n",
      "            The argument is directly passed as the ``compression``\n",
      "            argument to the underlying ``pyarrow.parquet.write_table()``\n",
      "            method (the default value \"snappy\" gets converted to uppercase).\n",
      "            https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow-parquet-write-table\n",
      "    \n",
      "            If the job config schema is missing, the argument is directly\n",
      "            passed as the ``compression`` argument to the underlying\n",
      "            ``DataFrame.to_parquet()`` method.\n",
      "            https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet\n",
      "        timeout:\n",
      "            The number of seconds to wait for the underlying HTTP transport\n",
      "            before using ``retry``. Depending on the retry strategy, a request may\n",
      "            be repeated several times using the same timeout each time.\n",
      "    \n",
      "            Can also be passed as a tuple (connect_timeout, read_timeout).\n",
      "            See :meth:`requests.Session.request` documentation for details.\n",
      "    \n",
      "    Returns:\n",
      "        google.cloud.bigquery.job.LoadJob: A new load job.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError:\n",
      "            If a usable parquet engine cannot be found. This method\n",
      "            requires :mod:`pyarrow` to be installed.\n",
      "        TypeError:\n",
      "            If ``job_config`` is not an instance of\n",
      "            :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.load_table_from_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doesn't work on free tier; requires billing\n",
    "\n",
    "sql = \"INSERT INTO `ds1_test.users2` SELECT * FROM `ds1_test.users2`\"\n",
    "query_job = client.query(sql).result()\n",
    "#query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Don</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id user_name\n",
       "0        1       Bob\n",
       "1        2      Bill\n",
       "2        3      Kate\n",
       "3        4      Matt\n",
       "4        5       Don"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM `ds1_test.users2` LIMIT 10'\n",
    "\n",
    "df = pd.read_gbq(query)\n",
    "df.head(5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
