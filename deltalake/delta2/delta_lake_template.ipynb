{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Matt Martin\n",
    "Date: 2023-10-20\n",
    "Desc: Delta Lake Template in jupypter nb for docker image to copy when loading for end user\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from delta import *\n",
    "\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(pyspark.sql.SparkSession.builder.appName(\"delta\")).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table_if_exists(tbl_path) -> None:\n",
    "    \n",
    "    from delta import DeltaTable\n",
    "\n",
    "    try:\n",
    "        # Load the Delta table\n",
    "        delta_table = DeltaTable.forPath(spark, tbl_path)\n",
    "\n",
    "        # Delete the Delta table\n",
    "        delta_table.delete()\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_replace_delta_table(df, tbl_path) -> None:\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(tbl_path)\n",
    "    except Exception as e:\n",
    "        df.write.format(\"delta\").save(tbl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_src_table() -> None:\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"hire_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        (\"Matt\", 20, datetime(2022,8,19)),\n",
    "        (\"Bill\", 35, datetime(2023,4,15)),\n",
    "        (\"Nancy\", 57, datetime(2022,4,23)),\n",
    "        (\"Rachel\", 19, datetime(2021,6,7)),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    create_or_replace_delta_table(df, \"/home/jovyan/src_ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the delta lake table\n",
    "build_src_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the table and display the results\n",
    "src_df = spark.read.format(\"delta\").load(\"/home/jovyan/src_ppl\")\n",
    "src_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets do some spark sql\n",
    "src_df.createTempView(\"persons\")\n",
    "spark.sql(\"SELECT * FROM persons\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update some data\n",
    "spark.sql(\"UPDATE persons set age = 20 where name = 'Rachel'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the table\n",
    "spark.sql(\"CREATE OR REPLACE TABLE persons2 USING delta location '/home/jovyan/ppl2' AS SELECT * FROM persons WHERE age BETWEEN 20 and 38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete test\n",
    "spark.sql(\"DELETE FROM persons2 WHERE name = 'Rachel'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert Test\n",
    "sql = \"\"\"\n",
    "INSERT INTO persons2 (name, age, hire_date)\n",
    "VALUES \n",
    "     ('Greg',42,'2023-01-01')\n",
    "    ,('Adam',31,'2023-08-05')\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge test\n",
    "sql = \"\"\"\n",
    "MERGE INTO persons as TGT\n",
    "    USING persons2 AS SRC\n",
    "        ON TGT.name = SRC.name\n",
    "    WHEN MATCHED THEN UPDATE\n",
    "        SET TGT.age = SRC.AGE, TGT.hire_date = SRC.hire_date\n",
    "    WHEN NOT MATCHED THEN \n",
    "        INSERT (name, age, hire_date)\n",
    "        VALUES (SRC.name, SRC.age, SRC.hire_date)\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect updated results\n",
    "spark.sql(\"SELECT * FROM persons\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
